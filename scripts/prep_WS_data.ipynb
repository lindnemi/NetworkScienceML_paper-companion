{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "if nx.__version__ == '2.8.8':\n",
    "    import warnings \n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from node_classification import node_categories\n",
    "names_node_cats = ['bulk', 'root', 'dense sprout', 'sparse sprout', 'inner tree node', 'proper leaf']\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_WS_grid_measures(G, A, P, nn, diffs, flows):\n",
    "    \n",
    "    loads = np.abs(flows)\n",
    "    max_load_idx = np.argmax(loads)\n",
    "    load_idx = list(list(G.edges)[max_load_idx])\n",
    "    \n",
    "    # For backup measures, i.e., post failure of maximally loaded line\n",
    "    G_post = copy.deepcopy(G)\n",
    "    G_post.remove_edge(*load_idx)\n",
    "    post_connected = nx.is_connected(G_post)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # node features\n",
    "    \n",
    "    df[\"node_number\"] = nn\n",
    "\n",
    "    df[\"P\"] = P\n",
    "    \n",
    "    df[\"AP\"] = A.dot(P)\n",
    "    \n",
    "    df[\"AAP\"] = A.dot(df[\"AP\"])\n",
    "    \n",
    "    df[\"AAAP\"] = A.dot(df[\"AAP\"])\n",
    "    \n",
    "    df[\"row_sum_AA\"] = A.dot(A.dot(np.ones(len(P))))\n",
    "    \n",
    "    df[\"row_sum_AAA\"] = A.dot(df[\"row_sum_AA\"])\n",
    "    \n",
    "    df[\"degree\"] = [d[1] for d in G.degree]\n",
    "    \n",
    "    df[\"degree_centrality\"] = nx.degree_centrality(G).values()\n",
    "    \n",
    "    df[\"neighbor_degree_min\"] = [min(df[\"degree\"][G.neighbors(node)]) for node in G.nodes]\n",
    "    \n",
    "    df[\"neighbor_degree_max\"] = [max(df[\"degree\"][G.neighbors(node)]) for node in G.nodes]\n",
    "    \n",
    "    df[\"neighbor_degree_mean\"] = [np.array(df[\"degree\"][G.neighbors(node)]).mean() for node in G.nodes]\n",
    "    \n",
    "    df[\"clustering\"] = nx.clustering(G).values()\n",
    "    \n",
    "    df[\"betweenness_centrality\"] = nx.betweenness_centrality(G).values()\n",
    "    \n",
    "    df[\"closeness_centrality\"] = nx.closeness_centrality(G).values()\n",
    "    \n",
    "    df[\"load_centrality\"] = nx.load_centrality(G).values()\n",
    "    \n",
    "    df[\"eigenvector_centrality\"] = nx.eigenvector_centrality(G,max_iter=10000).values()\n",
    "    \n",
    "    df[\"katz_centrality\"] = nx.katz_centrality(G).values()\n",
    "    \n",
    "    df[\"second_order_centrality\"] = nx.second_order_centrality(G).values()\n",
    "    \n",
    "    df[\"current_flow_closeness_centrality\"] = nx.current_flow_closeness_centrality(G).values()\n",
    "    \n",
    "    df[\"current_flow_betweenness_centrality\"] = nx.current_flow_betweenness_centrality(G).values()\n",
    "    \n",
    "    df[\"average_neighbor_degree\"] = nx.average_neighbor_degree(G).values()\n",
    "    \n",
    "    df[\"harmonic_centrality\"] = nx.harmonic_centrality(G).values()\n",
    "    \n",
    "    df[\"square_clustering\"] = nx.square_clustering(G).values()\n",
    "    \n",
    "    df[\"eccentricity\"] = nx.eccentricity(G).values()\n",
    "    \n",
    "    # This redundant feature is only here for backwards compatibility\n",
    "    df[\"node_cat\"] = node_categories(G, denseThres=5).values()\n",
    "    for cat in names_node_cats:\n",
    "#         ddf[cat] = [node == cat for node in df[\"node_cat\"]] -> df[cat] = [int(node == cat) for node in df[\"node_cat\"]\n",
    "        df[cat] = [int(node == cat) for node in df[\"node_cat\"]]\n",
    "\n",
    "        \n",
    "    # new node features\n",
    "    \n",
    "    df[\"fiedler_vector\"] = nx.fiedler_vector(G)\n",
    "    \n",
    "    connected_to_max_load_line = np.zeros(len(G), dtype=bool)\n",
    "    connected_to_max_load_line[load_idx] = True\n",
    "    df[\"node_connected_to_max_load_line\"] = connected_to_max_load_line\n",
    "    \n",
    "    # Assign an index to every edge, \n",
    "    # find indices of connected edges for very node (e[2]), \n",
    "    # use indices to look up corresponding loads\n",
    "    nx.set_edge_attributes(G, dict(zip(G.edges, range(len(G.edges())))), \"edge_idx\")\n",
    "    df[\"max_load_connected_lines\"] = [loads[[e[2] for e in G.edges(nbunch=node, data=\"edge_idx\")]].max() / 9.0 for node in G.nodes]\n",
    "    df[\"min_load_connected_lines\"] = [loads[[e[2] for e in G.edges(nbunch=node, data=\"edge_idx\")]].min() / 9.0 for node in G.nodes] \n",
    "    df[\"mean_load_connected_lines\"] = [loads[[e[2] for e in G.edges(nbunch=node, data=\"edge_idx\")]].mean() / 9.0 for node in G.nodes] \n",
    "    \n",
    "    # nx.set_edge_attributes(G, dict(zip(G.edges, 9 * np.cos(diffs))), \"laplace_weights\")\n",
    "    df[\"resistance_distance_centrality\"] = nx.current_flow_closeness_centrality(G, weight=\"laplace_weights\").values()\n",
    "    \n",
    "    # graph features\n",
    "    \n",
    "    N = len(G.nodes)\n",
    "    M = len(G.edges)\n",
    "    \n",
    "    df[\"degree_assortativity_coefficient\"]= nx.degree_assortativity_coefficient(G)\n",
    "    \n",
    "    df[\"transitivity\"] = nx.transitivity(G)\n",
    "    \n",
    "    df[\"diameter\"] = nx.diameter(G)\n",
    "    \n",
    "    nx.set_node_attributes(G, dict(zip(G.nodes, P)), \"P\")\n",
    "    \n",
    "    df[\"attribute_assortativity_coefficient_P\"] = nx.attribute_assortativity_coefficient(G, \"P\")\n",
    "    \n",
    "    df[\"kirchhoff_index\"] = np.reciprocal(df[\"current_flow_closeness_centrality\"]).mean()\n",
    "    \n",
    "    df[\"resistance_distance_kirchhoff_index\"] = np.reciprocal(df[\"resistance_distance_centrality\"]).mean()\n",
    "    \n",
    "    # new graph features\n",
    "    \n",
    "    spec = nx.laplacian_spectrum(G)\n",
    "    \n",
    "    df[\"inverse_algebraic_connectivity\"] = 1 / spec[1]\n",
    "    \n",
    "    df[\"eigenratio\"] = spec[1] / spec[-1]\n",
    "     \n",
    "    df[\"power_sign_ratio\"] = (sum([-P[e[0]] * P[e[1]] for e in G.edges]) + M) / (2 * M)\n",
    "    \n",
    "    df[\"maximal_line_load\"] = loads[max_load_idx] / 9.0\n",
    "    \n",
    "    df[\"universal_kuramoto_order_parameter\"] = np.cos(diffs).sum() / M\n",
    "    \n",
    "    df[\"connected_post\"] = post_connected\n",
    "    if post_connected:\n",
    "        # Post failure steady state in DC approximation, i.e, P = L theta\n",
    "        theta_post = sp.sparse.linalg.lsqr(nx.laplacian_matrix(G_post),P)[0]\n",
    "        theta_post = theta_post - theta_post[0] # choose first theta as 0\n",
    "        diffs_post = [theta_post[e[0]] - theta_post[e[1]] for e in G_post.edges]\n",
    "        loads_post = np.abs(9 * np.sin(diffs_post))      \n",
    "        # Line outage distribution factor (LODF) in DC approximation\n",
    "        df[\"maximal_line_load_post_dc\"] = loads_post.max() / 9.0\n",
    "        df[\"backup_capacity\"] = (loads_post - np.delete(loads, max_load_idx)).max()       \n",
    "    else:\n",
    "        df[\"maximal_line_load_post_dc\"] = np.nan\n",
    "        df[\"backup_capacity\"] = np.nan\n",
    "  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes = pd.read_csv(\"../WS_nodes.csv\")\n",
    "df_edges = pd.read_csv(\"../WS_edges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nodes = df_nodes[0:256]\n",
    "# df_edges = df_edges[0:4*256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_computed = pd.DataFrame()\n",
    "for nf in df_nodes.net_file.unique():\n",
    "    df_e = df_edges[df_edges[\"net_file\"] == nf]\n",
    "    df_n = df_nodes[df_nodes[\"net_file\"] == nf]\n",
    "    G = nx.empty_graph(64) # Create nodes first, so node indices appear in order. This is important for indexing into G.nodes for example\n",
    "    G.add_edges_from(zip(df_e.sources - 1, df_e.destinations -1)) # offset by 1 because Julia vs Python.\n",
    "    A = nx.adjacency_matrix(G)\n",
    "    P = df_n.P\n",
    "    nn = df_n.node_number\n",
    "    diffs = df_e.src_phi - df_e.dst_phi + df_e.alpha\n",
    "    flows = df_e.K * np.sin(diffs)\n",
    "    laplace_weights = df_e.K * np.cos(diffs)\n",
    "    nx.set_edge_attributes(G, dict(zip(G.edges, laplace_weights)), \"laplace_weights\")\n",
    "\n",
    "    df_n_c = compute_WS_grid_measures(G, A, P.values, nn.values, diffs.values, flows.values)\n",
    "    df_n_c[\"net_file\"] = nf\n",
    "\n",
    "    df_n_computed = pd.concat([df_n_computed, df_n_c])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_final = pd.merge(df_nodes, df_n_computed, on=[\"net_file\", \"node_number\"], how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"WS_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NetSciGNN",
   "language": "python",
   "name": "netscignn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
